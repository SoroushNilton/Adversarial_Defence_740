{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf sample_data/  \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp -r /content/drive/MyDrive/740_deeplearning/* /content/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CIFAR-100 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define variables separately\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "epochs = 100\n",
    "weight_decay = 2e-4\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "model_dir = './model-cifar-ResNet18'\n",
    "\n",
    "# Settings\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# Setup data loader for CIFAR-100\n",
    "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "testset  = torchvision.datasets.CIFAR100(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Train function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate loss\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation functions for train and test data\n",
    "def eval_model(model, device, data_loader, loader_type, epoch):\n",
    "    assert loader_type in ['train', 'test'], \"loader_type must be either 'train' or 'test'\"\n",
    "    model.eval()\n",
    "    data_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            data_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    data_loss /= len(data_loader.dataset)\n",
    "\n",
    "    print('Epoch: {}, {}: Average loss: {:.4f}, Accuracy: {:.0f}%'.format(epoch, \n",
    "                                                                          loader_type.capitalize(), \n",
    "                                                                          data_loss,\n",
    "                                                                          100. * correct / len(data_loader.dataset)))\n",
    "\n",
    "    data_accuracy = correct / len(data_loader.dataset)\n",
    "    return data_loss, data_accuracy\n",
    "\n",
    "\n",
    "def plot_metrics(train_losses, train_accuracies, test_losses, test_accuracies, dataset_name):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(model_dir, f'loss_accuracy_plot_{dataset_name}.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Adjust learning rate function\n",
    "def adjust_learning_rate(optimizer, epoch, initial_lr):\n",
    "    lr = initial_lr\n",
    "    if epoch >= 75:\n",
    "        lr = initial_lr * 0.1\n",
    "    if epoch >= 90:\n",
    "        lr = initial_lr * 0.01\n",
    "    if epoch >= 100:\n",
    "        lr = initial_lr * 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def main():\n",
    "    dataset_name = 'cifar100'\n",
    "    print(f\"Training on {dataset_name.upper()} dataset\")\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader  = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model = ResNet18().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    best_test_accuracy = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        adjust_learning_rate(optimizer, epoch, lr)\n",
    "\n",
    "        # Training and updating losses and accuracies\n",
    "        batch_train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "        print('================================================================')\n",
    "        avg_train_loss, train_accuracy = eval_model(model, device, train_loader, 'train', epoch)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Testing and updating losses and accuracies\n",
    "        avg_test_loss, test_accuracy = eval_model(model, device, test_loader, 'test', epoch)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    " \n",
    "\n",
    "        # Save the model if the test accuracy is the best seen so far\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, f'model-ResNet18-{dataset_name}-best_epoch.pt'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(model_dir, f'opt-ResNet18-{dataset_name}-checkpoint_best_epoch.tar'))\n",
    "\n",
    "    plot_metrics(train_losses, train_accuracies, test_losses, test_accuracies, dataset_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
